apiVersion: tekton.dev/v1beta1
kind: PipelineRun
metadata:
  annotations:
    pipelines.kubeflow.org/pipeline_spec: '{"description": "Predict which passengers
      survived the Titanic shipwreck", "name": "titanic-ml-gxj28"}'
    sidecar.istio.io/inject: 'false'
    tekton.dev/artifact_bucket: mlpipeline
    tekton.dev/artifact_endpoint: minio-service.kubeflow:9000
    tekton.dev/artifact_endpoint_scheme: http://
    tekton.dev/input_artifacts: '{"datapreprocessing": [{"name": "kale-marshal-volume-name",
      "parent_task": "kale-marshal-volume"}], "decisiontree": [{"name": "kale-marshal-volume-name",
      "parent_task": "kale-marshal-volume"}], "featureengineering": [{"name": "kale-marshal-volume-name",
      "parent_task": "kale-marshal-volume"}], "loaddata": [{"name": "kale-marshal-volume-name",
      "parent_task": "kale-marshal-volume"}], "logisticregression": [{"name": "kale-marshal-volume-name",
      "parent_task": "kale-marshal-volume"}], "naivebayes": [{"name": "kale-marshal-volume-name",
      "parent_task": "kale-marshal-volume"}], "randomforest": [{"name": "kale-marshal-volume-name",
      "parent_task": "kale-marshal-volume"}], "results": [{"name": "kale-marshal-volume-name",
      "parent_task": "kale-marshal-volume"}], "svm": [{"name": "kale-marshal-volume-name",
      "parent_task": "kale-marshal-volume"}]}'
    tekton.dev/output_artifacts: '{"datapreprocessing": [{"key": "artifacts/$PIPELINERUN/datapreprocessing/mlpipeline-ui-metadata.tgz",
      "name": "mlpipeline-ui-metadata", "path": "/mlpipeline-ui-metadata.json"}, {"key":
      "artifacts/$PIPELINERUN/datapreprocessing/datapreprocessing.tgz", "name": "datapreprocessing",
      "path": "/datapreprocessing.html"}], "decisiontree": [{"key": "artifacts/$PIPELINERUN/decisiontree/mlpipeline-ui-metadata.tgz",
      "name": "mlpipeline-ui-metadata", "path": "/mlpipeline-ui-metadata.json"}, {"key":
      "artifacts/$PIPELINERUN/decisiontree/decisiontree.tgz", "name": "decisiontree",
      "path": "/decisiontree.html"}], "featureengineering": [{"key": "artifacts/$PIPELINERUN/featureengineering/mlpipeline-ui-metadata.tgz",
      "name": "mlpipeline-ui-metadata", "path": "/mlpipeline-ui-metadata.json"}, {"key":
      "artifacts/$PIPELINERUN/featureengineering/featureengineering.tgz", "name":
      "featureengineering", "path": "/featureengineering.html"}], "loaddata": [{"key":
      "artifacts/$PIPELINERUN/loaddata/mlpipeline-ui-metadata.tgz", "name": "mlpipeline-ui-metadata",
      "path": "/mlpipeline-ui-metadata.json"}, {"key": "artifacts/$PIPELINERUN/loaddata/loaddata.tgz",
      "name": "loaddata", "path": "/loaddata.html"}], "logisticregression": [{"key":
      "artifacts/$PIPELINERUN/logisticregression/mlpipeline-ui-metadata.tgz", "name":
      "mlpipeline-ui-metadata", "path": "/mlpipeline-ui-metadata.json"}, {"key": "artifacts/$PIPELINERUN/logisticregression/logisticregression.tgz",
      "name": "logisticregression", "path": "/logisticregression.html"}], "naivebayes":
      [{"key": "artifacts/$PIPELINERUN/naivebayes/mlpipeline-ui-metadata.tgz", "name":
      "mlpipeline-ui-metadata", "path": "/mlpipeline-ui-metadata.json"}, {"key": "artifacts/$PIPELINERUN/naivebayes/naivebayes.tgz",
      "name": "naivebayes", "path": "/naivebayes.html"}], "randomforest": [{"key":
      "artifacts/$PIPELINERUN/randomforest/mlpipeline-ui-metadata.tgz", "name": "mlpipeline-ui-metadata",
      "path": "/mlpipeline-ui-metadata.json"}, {"key": "artifacts/$PIPELINERUN/randomforest/randomforest.tgz",
      "name": "randomforest", "path": "/randomforest.html"}], "results": [{"key":
      "artifacts/$PIPELINERUN/results/mlpipeline-ui-metadata.tgz", "name": "mlpipeline-ui-metadata",
      "path": "/mlpipeline-ui-metadata.json"}, {"key": "artifacts/$PIPELINERUN/results/results.tgz",
      "name": "results", "path": "/results.html"}], "svm": [{"key": "artifacts/$PIPELINERUN/svm/mlpipeline-ui-metadata.tgz",
      "name": "mlpipeline-ui-metadata", "path": "/mlpipeline-ui-metadata.json"}, {"key":
      "artifacts/$PIPELINERUN/svm/svm.tgz", "name": "svm", "path": "/svm.html"}]}'
  name: titanic-ml-gxj28
spec:
  pipelineSpec:
    tasks:
    - name: kale-marshal-volume
      params:
      - name: action
        value: create
      - name: output
        value: "- name: manifest\n  valueFrom: '{}'\n- name: name\n  valueFrom: '{.metadata.name}'\n\
          - name: size\n  valueFrom: '{.status.capacity.storage}'\n"
      - name: set-ownerreference
        value: 'false'
      taskSpec:
        params:
        - description: Action on the resource
          name: action
          type: string
        - default: strategic
          description: Merge strategy when using action patch
          name: merge-strategy
          type: string
        - default: ''
          description: An express to retrieval data from resource.
          name: output
          type: string
        - default: ''
          description: A label selector express to decide if the action on resource
            is success.
          name: success-condition
          type: string
        - default: ''
          description: A label selector express to decide if the action on resource
            is failure.
          name: failure-condition
          type: string
        - default: index.docker.io/aipipeline/kubeclient:v0.0.2
          description: Kubectl wrapper image
          name: image
          type: string
        - default: 'false'
          description: Enable set owner reference for created resource.
          name: set-ownerreference
          type: string
        results:
        - description: '{}'
          name: manifest
        - description: '{.metadata.name}'
          name: name
        - description: '{.status.capacity.storage}'
          name: size
        steps:
        - args:
          - --action=$(params.action)
          - --merge-strategy=$(params.merge-strategy)
          - "--manifest=apiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n \
            \ name: $(PIPELINERUN)-kale-marshal-pvc\nspec:\n  accessModes:\n  - ReadWriteMany\n\
            \  resources:\n    requests:\n      storage: 1Gi\n"
          - --output=$(params.output)
          - --success-condition=$(params.success-condition)
          - --failure-condition=$(params.failure-condition)
          - --set-ownerreference=$(params.set-ownerreference)
          env:
          - name: PIPELINERUN
            valueFrom:
              fieldRef:
                fieldPath: metadata.labels['tekton.dev/pipelineRun']
          image: $(params.image)
          name: main
          resources: {}
    - name: loaddata
      params:
      - name: kale-marshal-volume-name
        value: $(tasks.kale-marshal-volume.results.name)
      taskSpec:
        params:
        - name: kale-marshal-volume-name
        stepTemplate:
          volumeMounts:
          - mountPath: ''
            name: mlpipeline-ui-metadata
        steps:
        - command:
          - python3
          - -u
          - -c
          - "def loaddata():\n    from kale.utils import mlmd_utils as _kale_mlmd_utils\n\
            \    _kale_mlmd_utils.init_metadata()\n\n    block1 = '''\n    import\
            \ numpy as np\n    import pandas as pd\n    import seaborn as sns\n  \
            \  from matplotlib import pyplot as plt\n    from matplotlib import style\n\
            \n    from sklearn import linear_model\n    from sklearn.linear_model\
            \ import LogisticRegression\n    from sklearn.ensemble import RandomForestClassifier\n\
            \    from sklearn.linear_model import Perceptron\n    from sklearn.linear_model\
            \ import SGDClassifier\n    from sklearn.tree import DecisionTreeClassifier\n\
            \    from sklearn.neighbors import KNeighborsClassifier\n    from sklearn.svm\
            \ import SVC\n    from sklearn.naive_bayes import GaussianNB\n    '''\n\
            \n    block2 = '''\n    path = \"data/\"\n\n    PREDICTION_LABEL = 'Survived'\n\
            \n    test_df = pd.read_csv(path + \"test.csv\")\n    train_df = pd.read_csv(path\
            \ + \"train.csv\")\n    '''\n\n    data_saving_block = '''\n    # -----------------------DATA\
            \ SAVING START---------------------------------\n    from kale.marshal\
            \ import utils as _kale_marshal_utils\n    _kale_marshal_utils.set_kale_data_directory(\"\
            /marshal\")\n    _kale_marshal_utils.save(PREDICTION_LABEL, \"PREDICTION_LABEL\"\
            )\n    _kale_marshal_utils.save(test_df, \"test_df\")\n    _kale_marshal_utils.save(train_df,\
            \ \"train_df\")\n    # -----------------------DATA SAVING END-----------------------------------\n\
            \    '''\n\n    # run the code blocks inside a jupyter kernel\n    from\
            \ kale.utils.jupyter_utils import run_code as _kale_run_code\n    from\
            \ kale.utils.kfp_utils import \\\n        update_uimetadata as _kale_update_uimetadata\n\
            \    blocks = (\n        block1,\n        block2,\n        data_saving_block)\n\
            \    html_artifact = _kale_run_code(blocks)\n    with open(\"/loaddata.html\"\
            , \"w\") as f:\n        f.write(html_artifact)\n    _kale_update_uimetadata('loaddata')\n\
            \n    _kale_mlmd_utils.call(\"mark_execution_complete\")\n\nimport argparse\n\
            _parser = argparse.ArgumentParser(prog='Loaddata', description='')\n_parsed_args\
            \ = vars(_parser.parse_args())\n\n_outputs = loaddata(**_parsed_args)\n"
          image: python:3.7
          name: main
          securityContext:
            runAsUser: 0
          volumeMounts:
          - mountPath: /marshal
            name: kale-marshal-volume
          workingDir: /Users/animeshsingh/go/src/github.com/kubeflow/kale/examples/titanic-ml-dataset
        - env:
          - name: PIPELINERUN
            valueFrom:
              fieldRef:
                fieldPath: metadata.labels['tekton.dev/pipelineRun']
          - name: PIPELINETASK
            valueFrom:
              fieldRef:
                fieldPath: metadata.labels['tekton.dev/pipelineTask']
          - name: ARTIFACT_ENDPOINT_SCHEME
            valueFrom:
              fieldRef:
                fieldPath: metadata.annotations['tekton.dev/artifact_endpoint_scheme']
          - name: ARTIFACT_ENDPOINT
            valueFrom:
              fieldRef:
                fieldPath: metadata.annotations['tekton.dev/artifact_endpoint']
          - name: ARTIFACT_BUCKET
            valueFrom:
              fieldRef:
                fieldPath: metadata.annotations['tekton.dev/artifact_bucket']
          - name: AWS_ACCESS_KEY_ID
            valueFrom:
              secretKeyRef:
                key: accesskey
                name: mlpipeline-minio-artifact
          - name: AWS_SECRET_ACCESS_KEY
            valueFrom:
              secretKeyRef:
                key: secretkey
                name: mlpipeline-minio-artifact
          image: minio/mc
          name: copy-artifacts
          script: '#!/usr/bin/env sh

            mc config host add storage ${ARTIFACT_ENDPOINT_SCHEME}${ARTIFACT_ENDPOINT}
            $AWS_ACCESS_KEY_ID $AWS_SECRET_ACCESS_KEY

            tar -cvzf mlpipeline-ui-metadata.tgz /mlpipeline-ui-metadata.json

            mc cp mlpipeline-ui-metadata.tgz storage/$ARTIFACT_BUCKET/artifacts/$PIPELINERUN/$PIPELINETASK/mlpipeline-ui-metadata.tgz

            tar -cvzf loaddata.tgz /loaddata.html

            mc cp loaddata.tgz storage/$ARTIFACT_BUCKET/artifacts/$PIPELINERUN/$PIPELINETASK/loaddata.tgz

            '
        volumes:
        - name: kale-marshal-volume
          persistentVolumeClaim:
            claimName: $(inputs.params.kale-marshal-volume-name)
    - name: datapreprocessing
      params:
      - name: kale-marshal-volume-name
        value: $(tasks.kale-marshal-volume.results.name)
      runAfter:
      - loaddata
      taskSpec:
        params:
        - name: kale-marshal-volume-name
        stepTemplate:
          volumeMounts:
          - mountPath: ''
            name: mlpipeline-ui-metadata
        steps:
        - command:
          - python3
          - -u
          - -c
          - "def datapreprocessing():\n    from kale.utils import mlmd_utils as _kale_mlmd_utils\n\
            \    _kale_mlmd_utils.init_metadata()\n\n    data_loading_block = '''\n\
            \    # -----------------------DATA LOADING START--------------------------------\n\
            \    from kale.marshal import utils as _kale_marshal_utils\n    _kale_marshal_utils.set_kale_data_directory(\"\
            /marshal\")\n    _kale_marshal_utils.set_kale_directory_file_names()\n\
            \    test_df = _kale_marshal_utils.load(\"test_df\")\n    train_df = _kale_marshal_utils.load(\"\
            train_df\")\n    # -----------------------DATA LOADING END----------------------------------\n\
            \    '''\n\n    block1 = '''\n    import numpy as np\n    import pandas\
            \ as pd\n    import seaborn as sns\n    from matplotlib import pyplot\
            \ as plt\n    from matplotlib import style\n\n    from sklearn import\
            \ linear_model\n    from sklearn.linear_model import LogisticRegression\n\
            \    from sklearn.ensemble import RandomForestClassifier\n    from sklearn.linear_model\
            \ import Perceptron\n    from sklearn.linear_model import SGDClassifier\n\
            \    from sklearn.tree import DecisionTreeClassifier\n    from sklearn.neighbors\
            \ import KNeighborsClassifier\n    from sklearn.svm import SVC\n    from\
            \ sklearn.naive_bayes import GaussianNB\n    '''\n\n    block2 = '''\n\
            \    data = [train_df, test_df]\n    for dataset in data:\n        dataset['relatives']\
            \ = dataset['SibSp'] + dataset['Parch']\n        dataset.loc[dataset['relatives']\
            \ > 0, 'not_alone'] = 0\n        dataset.loc[dataset['relatives'] == 0,\
            \ 'not_alone'] = 1\n        dataset['not_alone'] = dataset['not_alone'].astype(int)\n\
            \    train_df['not_alone'].value_counts()\n    '''\n\n    block3 = '''\n\
            \    # This does not contribute to a person survival probability\n   \
            \ train_df = train_df.drop(['PassengerId'], axis=1)\n    '''\n\n    block4\
            \ = '''\n    import re\n    deck = {\"A\": 1, \"B\": 2, \"C\": 3, \"D\"\
            : 4, \"E\": 5, \"F\": 6, \"G\": 7, \"U\": 8}\n    data = [train_df, test_df]\n\
            \n    for dataset in data:\n        dataset['Cabin'] = dataset['Cabin'].fillna(\"\
            U0\")\n        dataset['Deck'] = dataset['Cabin'].map(lambda x: re.compile(\"\
            ([a-zA-Z]+)\").search(x).group())\n        dataset['Deck'] = dataset['Deck'].map(deck)\n\
            \        dataset['Deck'] = dataset['Deck'].fillna(0)\n        dataset['Deck']\
            \ = dataset['Deck'].astype(int)\n    # we can now drop the cabin feature\n\
            \    train_df = train_df.drop(['Cabin'], axis=1)\n    test_df = test_df.drop(['Cabin'],\
            \ axis=1)\n    '''\n\n    block5 = '''\n    data = [train_df, test_df]\n\
            \n    for dataset in data:\n        mean = train_df[\"Age\"].mean()\n\
            \        std = test_df[\"Age\"].std()\n        is_null = dataset[\"Age\"\
            ].isnull().sum()\n        # compute random numbers between the mean, std\
            \ and is_null\n        rand_age = np.random.randint(mean - std, mean +\
            \ std, size = is_null)\n        # fill NaN values in Age column with random\
            \ values generated\n        age_slice = dataset[\"Age\"].copy()\n    \
            \    age_slice[np.isnan(age_slice)] = rand_age\n        dataset[\"Age\"\
            ] = age_slice\n        dataset[\"Age\"] = train_df[\"Age\"].astype(int)\n\
            \    train_df[\"Age\"].isnull().sum()\n    '''\n\n    block6 = '''\n \
            \   train_df['Embarked'].describe()\n    '''\n\n    block7 = '''\n   \
            \ # fill with most common value\n    common_value = 'S'\n    data = [train_df,\
            \ test_df]\n\n    for dataset in data:\n        dataset['Embarked'] =\
            \ dataset['Embarked'].fillna(common_value)\n    '''\n\n    block8 = '''\n\
            \    train_df.info()\n    '''\n\n    data_saving_block = '''\n    # -----------------------DATA\
            \ SAVING START---------------------------------\n    from kale.marshal\
            \ import utils as _kale_marshal_utils\n    _kale_marshal_utils.set_kale_data_directory(\"\
            /marshal\")\n    _kale_marshal_utils.save(test_df, \"test_df\")\n    _kale_marshal_utils.save(train_df,\
            \ \"train_df\")\n    # -----------------------DATA SAVING END-----------------------------------\n\
            \    '''\n\n    # run the code blocks inside a jupyter kernel\n    from\
            \ kale.utils.jupyter_utils import run_code as _kale_run_code\n    from\
            \ kale.utils.kfp_utils import \\\n        update_uimetadata as _kale_update_uimetadata\n\
            \    blocks = (data_loading_block,\n              block1,\n          \
            \    block2,\n              block3,\n              block4,\n         \
            \     block5,\n              block6,\n              block7,\n        \
            \      block8,\n              data_saving_block)\n    html_artifact =\
            \ _kale_run_code(blocks)\n    with open(\"/datapreprocessing.html\", \"\
            w\") as f:\n        f.write(html_artifact)\n    _kale_update_uimetadata('datapreprocessing')\n\
            \n    _kale_mlmd_utils.call(\"mark_execution_complete\")\n\nimport argparse\n\
            _parser = argparse.ArgumentParser(prog='Datapreprocessing', description='')\n\
            _parsed_args = vars(_parser.parse_args())\n\n_outputs = datapreprocessing(**_parsed_args)\n"
          image: python:3.7
          name: main
          securityContext:
            runAsUser: 0
          volumeMounts:
          - mountPath: /marshal
            name: kale-marshal-volume
          workingDir: /Users/animeshsingh/go/src/github.com/kubeflow/kale/examples/titanic-ml-dataset
        - env:
          - name: PIPELINERUN
            valueFrom:
              fieldRef:
                fieldPath: metadata.labels['tekton.dev/pipelineRun']
          - name: PIPELINETASK
            valueFrom:
              fieldRef:
                fieldPath: metadata.labels['tekton.dev/pipelineTask']
          - name: ARTIFACT_ENDPOINT_SCHEME
            valueFrom:
              fieldRef:
                fieldPath: metadata.annotations['tekton.dev/artifact_endpoint_scheme']
          - name: ARTIFACT_ENDPOINT
            valueFrom:
              fieldRef:
                fieldPath: metadata.annotations['tekton.dev/artifact_endpoint']
          - name: ARTIFACT_BUCKET
            valueFrom:
              fieldRef:
                fieldPath: metadata.annotations['tekton.dev/artifact_bucket']
          - name: AWS_ACCESS_KEY_ID
            valueFrom:
              secretKeyRef:
                key: accesskey
                name: mlpipeline-minio-artifact
          - name: AWS_SECRET_ACCESS_KEY
            valueFrom:
              secretKeyRef:
                key: secretkey
                name: mlpipeline-minio-artifact
          image: minio/mc
          name: copy-artifacts
          script: '#!/usr/bin/env sh

            mc config host add storage ${ARTIFACT_ENDPOINT_SCHEME}${ARTIFACT_ENDPOINT}
            $AWS_ACCESS_KEY_ID $AWS_SECRET_ACCESS_KEY

            tar -cvzf mlpipeline-ui-metadata.tgz /mlpipeline-ui-metadata.json

            mc cp mlpipeline-ui-metadata.tgz storage/$ARTIFACT_BUCKET/artifacts/$PIPELINERUN/$PIPELINETASK/mlpipeline-ui-metadata.tgz

            tar -cvzf datapreprocessing.tgz /datapreprocessing.html

            mc cp datapreprocessing.tgz storage/$ARTIFACT_BUCKET/artifacts/$PIPELINERUN/$PIPELINETASK/datapreprocessing.tgz

            '
        volumes:
        - name: kale-marshal-volume
          persistentVolumeClaim:
            claimName: $(inputs.params.kale-marshal-volume-name)
    - name: featureengineering
      params:
      - name: kale-marshal-volume-name
        value: $(tasks.kale-marshal-volume.results.name)
      runAfter:
      - datapreprocessing
      taskSpec:
        params:
        - name: kale-marshal-volume-name
        stepTemplate:
          volumeMounts:
          - mountPath: ''
            name: mlpipeline-ui-metadata
        steps:
        - command:
          - python3
          - -u
          - -c
          - "def featureengineering():\n    from kale.utils import mlmd_utils as _kale_mlmd_utils\n\
            \    _kale_mlmd_utils.init_metadata()\n\n    data_loading_block = '''\n\
            \    # -----------------------DATA LOADING START--------------------------------\n\
            \    from kale.marshal import utils as _kale_marshal_utils\n    _kale_marshal_utils.set_kale_data_directory(\"\
            /marshal\")\n    _kale_marshal_utils.set_kale_directory_file_names()\n\
            \    PREDICTION_LABEL = _kale_marshal_utils.load(\"PREDICTION_LABEL\"\
            )\n    test_df = _kale_marshal_utils.load(\"test_df\")\n    train_df =\
            \ _kale_marshal_utils.load(\"train_df\")\n    # -----------------------DATA\
            \ LOADING END----------------------------------\n    '''\n\n    block1\
            \ = '''\n    import numpy as np\n    import pandas as pd\n    import seaborn\
            \ as sns\n    from matplotlib import pyplot as plt\n    from matplotlib\
            \ import style\n\n    from sklearn import linear_model\n    from sklearn.linear_model\
            \ import LogisticRegression\n    from sklearn.ensemble import RandomForestClassifier\n\
            \    from sklearn.linear_model import Perceptron\n    from sklearn.linear_model\
            \ import SGDClassifier\n    from sklearn.tree import DecisionTreeClassifier\n\
            \    from sklearn.neighbors import KNeighborsClassifier\n    from sklearn.svm\
            \ import SVC\n    from sklearn.naive_bayes import GaussianNB\n    '''\n\
            \n    block2 = '''\n    data = [train_df, test_df]\n\n    for dataset\
            \ in data:\n        dataset['Fare'] = dataset['Fare'].fillna(0)\n    \
            \    dataset['Fare'] = dataset['Fare'].astype(int)\n    '''\n\n    block3\
            \ = '''\n    data = [train_df, test_df]\n    titles = {\"Mr\": 1, \"Miss\"\
            : 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\n\n    for dataset in data:\n\
            \        # extract titles\n        dataset['Title'] = dataset.Name.str.extract('\
            \ ([A-Za-z]+)\\\\.', expand=False)\n        # replace titles with a more\
            \ common title or as Rare\n        dataset['Title'] = dataset['Title'].replace(['Lady',\
            \ 'Countess','Capt', 'Col','Don', 'Dr',\\\\\n                        \
            \                        'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n\
            \        dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n\
            \        dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n \
            \       dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n  \
            \      # convert titles into numbers\n        dataset['Title'] = dataset['Title'].map(titles)\n\
            \        # filling NaN with 0, to get safe\n        dataset['Title'] =\
            \ dataset['Title'].fillna(0)\n    train_df = train_df.drop(['Name'], axis=1)\n\
            \    test_df = test_df.drop(['Name'], axis=1)\n    '''\n\n    block4 =\
            \ '''\n    genders = {\"male\": 0, \"female\": 1}\n    data = [train_df,\
            \ test_df]\n\n    for dataset in data:\n        dataset['Sex'] = dataset['Sex'].map(genders)\n\
            \    '''\n\n    block5 = '''\n    train_df = train_df.drop(['Ticket'],\
            \ axis=1)\n    test_df = test_df.drop(['Ticket'], axis=1)\n    '''\n\n\
            \    block6 = '''\n    ports = {\"S\": 0, \"C\": 1, \"Q\": 2}\n    data\
            \ = [train_df, test_df]\n\n    for dataset in data:\n        dataset['Embarked']\
            \ = dataset['Embarked'].map(ports)\n    '''\n\n    block7 = '''\n    data\
            \ = [train_df, test_df]\n    for dataset in data:\n        dataset['Age']\
            \ = dataset['Age'].astype(int)\n        dataset.loc[ dataset['Age'] <=\
            \ 11, 'Age'] = 0\n        dataset.loc[(dataset['Age'] > 11) & (dataset['Age']\
            \ <= 18), 'Age'] = 1\n        dataset.loc[(dataset['Age'] > 18) & (dataset['Age']\
            \ <= 22), 'Age'] = 2\n        dataset.loc[(dataset['Age'] > 22) & (dataset['Age']\
            \ <= 27), 'Age'] = 3\n        dataset.loc[(dataset['Age'] > 27) & (dataset['Age']\
            \ <= 33), 'Age'] = 4\n        dataset.loc[(dataset['Age'] > 33) & (dataset['Age']\
            \ <= 40), 'Age'] = 5\n        dataset.loc[(dataset['Age'] > 40) & (dataset['Age']\
            \ <= 66), 'Age'] = 6\n        dataset.loc[ dataset['Age'] > 66, 'Age']\
            \ = 6\n\n    # let's see how it's distributed train_df['Age'].value_counts()\n\
            \    '''\n\n    block8 = '''\n    data = [train_df, test_df]\n\n    for\
            \ dataset in data:\n        dataset.loc[ dataset['Fare'] <= 7.91, 'Fare']\
            \ = 0\n        dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare']\
            \ <= 14.454), 'Fare'] = 1\n        dataset.loc[(dataset['Fare'] > 14.454)\
            \ & (dataset['Fare'] <= 31), 'Fare']   = 2\n        dataset.loc[(dataset['Fare']\
            \ > 31) & (dataset['Fare'] <= 99), 'Fare']   = 3\n        dataset.loc[(dataset['Fare']\
            \ > 99) & (dataset['Fare'] <= 250), 'Fare']   = 4\n        dataset.loc[\
            \ dataset['Fare'] > 250, 'Fare'] = 5\n        dataset['Fare'] = dataset['Fare'].astype(int)\n\
            \    '''\n\n    block9 = '''\n    data = [train_df, test_df]\n    for\
            \ dataset in data:\n        dataset['Age_Class']= dataset['Age']* dataset['Pclass']\n\
            \    '''\n\n    block10 = '''\n    for dataset in data:\n        dataset['Fare_Per_Person']\
            \ = dataset['Fare']/(dataset['relatives']+1)\n        dataset['Fare_Per_Person']\
            \ = dataset['Fare_Per_Person'].astype(int)\n    # Let's take a last look\
            \ at the training set, before we start training the models.\n    train_df.head(10)\n\
            \    '''\n\n    block11 = '''\n    train_labels = train_df[PREDICTION_LABEL]\n\
            \    train_df = train_df.drop(PREDICTION_LABEL, axis=1)\n    '''\n\n \
            \   data_saving_block = '''\n    # -----------------------DATA SAVING\
            \ START---------------------------------\n    from kale.marshal import\
            \ utils as _kale_marshal_utils\n    _kale_marshal_utils.set_kale_data_directory(\"\
            /marshal\")\n    _kale_marshal_utils.save(train_df, \"train_df\")\n  \
            \  _kale_marshal_utils.save(train_labels, \"train_labels\")\n    # -----------------------DATA\
            \ SAVING END-----------------------------------\n    '''\n\n    # run\
            \ the code blocks inside a jupyter kernel\n    from kale.utils.jupyter_utils\
            \ import run_code as _kale_run_code\n    from kale.utils.kfp_utils import\
            \ \\\n        update_uimetadata as _kale_update_uimetadata\n    blocks\
            \ = (data_loading_block,\n              block1,\n              block2,\n\
            \              block3,\n              block4,\n              block5,\n\
            \              block6,\n              block7,\n              block8,\n\
            \              block9,\n              block10,\n              block11,\n\
            \              data_saving_block)\n    html_artifact = _kale_run_code(blocks)\n\
            \    with open(\"/featureengineering.html\", \"w\") as f:\n        f.write(html_artifact)\n\
            \    _kale_update_uimetadata('featureengineering')\n\n    _kale_mlmd_utils.call(\"\
            mark_execution_complete\")\n\nimport argparse\n_parser = argparse.ArgumentParser(prog='Featureengineering',\
            \ description='')\n_parsed_args = vars(_parser.parse_args())\n\n_outputs\
            \ = featureengineering(**_parsed_args)\n"
          image: python:3.7
          name: main
          securityContext:
            runAsUser: 0
          volumeMounts:
          - mountPath: /marshal
            name: kale-marshal-volume
          workingDir: /Users/animeshsingh/go/src/github.com/kubeflow/kale/examples/titanic-ml-dataset
        - env:
          - name: PIPELINERUN
            valueFrom:
              fieldRef:
                fieldPath: metadata.labels['tekton.dev/pipelineRun']
          - name: PIPELINETASK
            valueFrom:
              fieldRef:
                fieldPath: metadata.labels['tekton.dev/pipelineTask']
          - name: ARTIFACT_ENDPOINT_SCHEME
            valueFrom:
              fieldRef:
                fieldPath: metadata.annotations['tekton.dev/artifact_endpoint_scheme']
          - name: ARTIFACT_ENDPOINT
            valueFrom:
              fieldRef:
                fieldPath: metadata.annotations['tekton.dev/artifact_endpoint']
          - name: ARTIFACT_BUCKET
            valueFrom:
              fieldRef:
                fieldPath: metadata.annotations['tekton.dev/artifact_bucket']
          - name: AWS_ACCESS_KEY_ID
            valueFrom:
              secretKeyRef:
                key: accesskey
                name: mlpipeline-minio-artifact
          - name: AWS_SECRET_ACCESS_KEY
            valueFrom:
              secretKeyRef:
                key: secretkey
                name: mlpipeline-minio-artifact
          image: minio/mc
          name: copy-artifacts
          script: '#!/usr/bin/env sh

            mc config host add storage ${ARTIFACT_ENDPOINT_SCHEME}${ARTIFACT_ENDPOINT}
            $AWS_ACCESS_KEY_ID $AWS_SECRET_ACCESS_KEY

            tar -cvzf mlpipeline-ui-metadata.tgz /mlpipeline-ui-metadata.json

            mc cp mlpipeline-ui-metadata.tgz storage/$ARTIFACT_BUCKET/artifacts/$PIPELINERUN/$PIPELINETASK/mlpipeline-ui-metadata.tgz

            tar -cvzf featureengineering.tgz /featureengineering.html

            mc cp featureengineering.tgz storage/$ARTIFACT_BUCKET/artifacts/$PIPELINERUN/$PIPELINETASK/featureengineering.tgz

            '
        volumes:
        - name: kale-marshal-volume
          persistentVolumeClaim:
            claimName: $(inputs.params.kale-marshal-volume-name)
    - name: decisiontree
      params:
      - name: kale-marshal-volume-name
        value: $(tasks.kale-marshal-volume.results.name)
      runAfter:
      - featureengineering
      taskSpec:
        params:
        - name: kale-marshal-volume-name
        stepTemplate:
          volumeMounts:
          - mountPath: ''
            name: mlpipeline-ui-metadata
        steps:
        - command:
          - python3
          - -u
          - -c
          - "def decisiontree():\n    from kale.utils import mlmd_utils as _kale_mlmd_utils\n\
            \    _kale_mlmd_utils.init_metadata()\n\n    data_loading_block = '''\n\
            \    # -----------------------DATA LOADING START--------------------------------\n\
            \    from kale.marshal import utils as _kale_marshal_utils\n    _kale_marshal_utils.set_kale_data_directory(\"\
            /marshal\")\n    _kale_marshal_utils.set_kale_directory_file_names()\n\
            \    train_df = _kale_marshal_utils.load(\"train_df\")\n    train_labels\
            \ = _kale_marshal_utils.load(\"train_labels\")\n    # -----------------------DATA\
            \ LOADING END----------------------------------\n    '''\n\n    block1\
            \ = '''\n    import numpy as np\n    import pandas as pd\n    import seaborn\
            \ as sns\n    from matplotlib import pyplot as plt\n    from matplotlib\
            \ import style\n\n    from sklearn import linear_model\n    from sklearn.linear_model\
            \ import LogisticRegression\n    from sklearn.ensemble import RandomForestClassifier\n\
            \    from sklearn.linear_model import Perceptron\n    from sklearn.linear_model\
            \ import SGDClassifier\n    from sklearn.tree import DecisionTreeClassifier\n\
            \    from sklearn.neighbors import KNeighborsClassifier\n    from sklearn.svm\
            \ import SVC\n    from sklearn.naive_bayes import GaussianNB\n    '''\n\
            \n    block2 = '''\n    decision_tree = DecisionTreeClassifier()\n   \
            \ decision_tree.fit(train_df, train_labels)\n    acc_decision_tree = round(decision_tree.score(train_df,\
            \ train_labels) * 100, 2)\n    '''\n\n    data_saving_block = '''\n  \
            \  # -----------------------DATA SAVING START---------------------------------\n\
            \    from kale.marshal import utils as _kale_marshal_utils\n    _kale_marshal_utils.set_kale_data_directory(\"\
            /marshal\")\n    _kale_marshal_utils.save(acc_decision_tree, \"acc_decision_tree\"\
            )\n    # -----------------------DATA SAVING END-----------------------------------\n\
            \    '''\n\n    # run the code blocks inside a jupyter kernel\n    from\
            \ kale.utils.jupyter_utils import run_code as _kale_run_code\n    from\
            \ kale.utils.kfp_utils import \\\n        update_uimetadata as _kale_update_uimetadata\n\
            \    blocks = (data_loading_block,\n              block1,\n          \
            \    block2,\n              data_saving_block)\n    html_artifact = _kale_run_code(blocks)\n\
            \    with open(\"/decisiontree.html\", \"w\") as f:\n        f.write(html_artifact)\n\
            \    _kale_update_uimetadata('decisiontree')\n\n    _kale_mlmd_utils.call(\"\
            mark_execution_complete\")\n\nimport argparse\n_parser = argparse.ArgumentParser(prog='Decisiontree',\
            \ description='')\n_parsed_args = vars(_parser.parse_args())\n\n_outputs\
            \ = decisiontree(**_parsed_args)\n"
          image: python:3.7
          name: main
          securityContext:
            runAsUser: 0
          volumeMounts:
          - mountPath: /marshal
            name: kale-marshal-volume
          workingDir: /Users/animeshsingh/go/src/github.com/kubeflow/kale/examples/titanic-ml-dataset
        - env:
          - name: PIPELINERUN
            valueFrom:
              fieldRef:
                fieldPath: metadata.labels['tekton.dev/pipelineRun']
          - name: PIPELINETASK
            valueFrom:
              fieldRef:
                fieldPath: metadata.labels['tekton.dev/pipelineTask']
          - name: ARTIFACT_ENDPOINT_SCHEME
            valueFrom:
              fieldRef:
                fieldPath: metadata.annotations['tekton.dev/artifact_endpoint_scheme']
          - name: ARTIFACT_ENDPOINT
            valueFrom:
              fieldRef:
                fieldPath: metadata.annotations['tekton.dev/artifact_endpoint']
          - name: ARTIFACT_BUCKET
            valueFrom:
              fieldRef:
                fieldPath: metadata.annotations['tekton.dev/artifact_bucket']
          - name: AWS_ACCESS_KEY_ID
            valueFrom:
              secretKeyRef:
                key: accesskey
                name: mlpipeline-minio-artifact
          - name: AWS_SECRET_ACCESS_KEY
            valueFrom:
              secretKeyRef:
                key: secretkey
                name: mlpipeline-minio-artifact
          image: minio/mc
          name: copy-artifacts
          script: '#!/usr/bin/env sh

            mc config host add storage ${ARTIFACT_ENDPOINT_SCHEME}${ARTIFACT_ENDPOINT}
            $AWS_ACCESS_KEY_ID $AWS_SECRET_ACCESS_KEY

            tar -cvzf mlpipeline-ui-metadata.tgz /mlpipeline-ui-metadata.json

            mc cp mlpipeline-ui-metadata.tgz storage/$ARTIFACT_BUCKET/artifacts/$PIPELINERUN/$PIPELINETASK/mlpipeline-ui-metadata.tgz

            tar -cvzf decisiontree.tgz /decisiontree.html

            mc cp decisiontree.tgz storage/$ARTIFACT_BUCKET/artifacts/$PIPELINERUN/$PIPELINETASK/decisiontree.tgz

            '
        volumes:
        - name: kale-marshal-volume
          persistentVolumeClaim:
            claimName: $(inputs.params.kale-marshal-volume-name)
    - name: svm
      params:
      - name: kale-marshal-volume-name
        value: $(tasks.kale-marshal-volume.results.name)
      runAfter:
      - featureengineering
      taskSpec:
        params:
        - name: kale-marshal-volume-name
        stepTemplate:
          volumeMounts:
          - mountPath: ''
            name: mlpipeline-ui-metadata
        steps:
        - command:
          - python3
          - -u
          - -c
          - "def svm():\n    from kale.utils import mlmd_utils as _kale_mlmd_utils\n\
            \    _kale_mlmd_utils.init_metadata()\n\n    data_loading_block = '''\n\
            \    # -----------------------DATA LOADING START--------------------------------\n\
            \    from kale.marshal import utils as _kale_marshal_utils\n    _kale_marshal_utils.set_kale_data_directory(\"\
            /marshal\")\n    _kale_marshal_utils.set_kale_directory_file_names()\n\
            \    train_df = _kale_marshal_utils.load(\"train_df\")\n    train_labels\
            \ = _kale_marshal_utils.load(\"train_labels\")\n    # -----------------------DATA\
            \ LOADING END----------------------------------\n    '''\n\n    block1\
            \ = '''\n    import numpy as np\n    import pandas as pd\n    import seaborn\
            \ as sns\n    from matplotlib import pyplot as plt\n    from matplotlib\
            \ import style\n\n    from sklearn import linear_model\n    from sklearn.linear_model\
            \ import LogisticRegression\n    from sklearn.ensemble import RandomForestClassifier\n\
            \    from sklearn.linear_model import Perceptron\n    from sklearn.linear_model\
            \ import SGDClassifier\n    from sklearn.tree import DecisionTreeClassifier\n\
            \    from sklearn.neighbors import KNeighborsClassifier\n    from sklearn.svm\
            \ import SVC\n    from sklearn.naive_bayes import GaussianNB\n    '''\n\
            \n    block2 = '''\n    linear_svc = SVC(gamma='auto')\n    linear_svc.fit(train_df,\
            \ train_labels)\n    acc_linear_svc = round(linear_svc.score(train_df,\
            \ train_labels) * 100, 2)\n    '''\n\n    data_saving_block = '''\n  \
            \  # -----------------------DATA SAVING START---------------------------------\n\
            \    from kale.marshal import utils as _kale_marshal_utils\n    _kale_marshal_utils.set_kale_data_directory(\"\
            /marshal\")\n    _kale_marshal_utils.save(acc_linear_svc, \"acc_linear_svc\"\
            )\n    # -----------------------DATA SAVING END-----------------------------------\n\
            \    '''\n\n    # run the code blocks inside a jupyter kernel\n    from\
            \ kale.utils.jupyter_utils import run_code as _kale_run_code\n    from\
            \ kale.utils.kfp_utils import \\\n        update_uimetadata as _kale_update_uimetadata\n\
            \    blocks = (data_loading_block,\n              block1,\n          \
            \    block2,\n              data_saving_block)\n    html_artifact = _kale_run_code(blocks)\n\
            \    with open(\"/svm.html\", \"w\") as f:\n        f.write(html_artifact)\n\
            \    _kale_update_uimetadata('svm')\n\n    _kale_mlmd_utils.call(\"mark_execution_complete\"\
            )\n\nimport argparse\n_parser = argparse.ArgumentParser(prog='Svm', description='')\n\
            _parsed_args = vars(_parser.parse_args())\n\n_outputs = svm(**_parsed_args)\n"
          image: python:3.7
          name: main
          securityContext:
            runAsUser: 0
          volumeMounts:
          - mountPath: /marshal
            name: kale-marshal-volume
          workingDir: /Users/animeshsingh/go/src/github.com/kubeflow/kale/examples/titanic-ml-dataset
        - env:
          - name: PIPELINERUN
            valueFrom:
              fieldRef:
                fieldPath: metadata.labels['tekton.dev/pipelineRun']
          - name: PIPELINETASK
            valueFrom:
              fieldRef:
                fieldPath: metadata.labels['tekton.dev/pipelineTask']
          - name: ARTIFACT_ENDPOINT_SCHEME
            valueFrom:
              fieldRef:
                fieldPath: metadata.annotations['tekton.dev/artifact_endpoint_scheme']
          - name: ARTIFACT_ENDPOINT
            valueFrom:
              fieldRef:
                fieldPath: metadata.annotations['tekton.dev/artifact_endpoint']
          - name: ARTIFACT_BUCKET
            valueFrom:
              fieldRef:
                fieldPath: metadata.annotations['tekton.dev/artifact_bucket']
          - name: AWS_ACCESS_KEY_ID
            valueFrom:
              secretKeyRef:
                key: accesskey
                name: mlpipeline-minio-artifact
          - name: AWS_SECRET_ACCESS_KEY
            valueFrom:
              secretKeyRef:
                key: secretkey
                name: mlpipeline-minio-artifact
          image: minio/mc
          name: copy-artifacts
          script: '#!/usr/bin/env sh

            mc config host add storage ${ARTIFACT_ENDPOINT_SCHEME}${ARTIFACT_ENDPOINT}
            $AWS_ACCESS_KEY_ID $AWS_SECRET_ACCESS_KEY

            tar -cvzf mlpipeline-ui-metadata.tgz /mlpipeline-ui-metadata.json

            mc cp mlpipeline-ui-metadata.tgz storage/$ARTIFACT_BUCKET/artifacts/$PIPELINERUN/$PIPELINETASK/mlpipeline-ui-metadata.tgz

            tar -cvzf svm.tgz /svm.html

            mc cp svm.tgz storage/$ARTIFACT_BUCKET/artifacts/$PIPELINERUN/$PIPELINETASK/svm.tgz

            '
        volumes:
        - name: kale-marshal-volume
          persistentVolumeClaim:
            claimName: $(inputs.params.kale-marshal-volume-name)
    - name: naivebayes
      params:
      - name: kale-marshal-volume-name
        value: $(tasks.kale-marshal-volume.results.name)
      runAfter:
      - featureengineering
      taskSpec:
        params:
        - name: kale-marshal-volume-name
        stepTemplate:
          volumeMounts:
          - mountPath: ''
            name: mlpipeline-ui-metadata
        steps:
        - command:
          - python3
          - -u
          - -c
          - "def naivebayes():\n    from kale.utils import mlmd_utils as _kale_mlmd_utils\n\
            \    _kale_mlmd_utils.init_metadata()\n\n    data_loading_block = '''\n\
            \    # -----------------------DATA LOADING START--------------------------------\n\
            \    from kale.marshal import utils as _kale_marshal_utils\n    _kale_marshal_utils.set_kale_data_directory(\"\
            /marshal\")\n    _kale_marshal_utils.set_kale_directory_file_names()\n\
            \    train_df = _kale_marshal_utils.load(\"train_df\")\n    train_labels\
            \ = _kale_marshal_utils.load(\"train_labels\")\n    # -----------------------DATA\
            \ LOADING END----------------------------------\n    '''\n\n    block1\
            \ = '''\n    import numpy as np\n    import pandas as pd\n    import seaborn\
            \ as sns\n    from matplotlib import pyplot as plt\n    from matplotlib\
            \ import style\n\n    from sklearn import linear_model\n    from sklearn.linear_model\
            \ import LogisticRegression\n    from sklearn.ensemble import RandomForestClassifier\n\
            \    from sklearn.linear_model import Perceptron\n    from sklearn.linear_model\
            \ import SGDClassifier\n    from sklearn.tree import DecisionTreeClassifier\n\
            \    from sklearn.neighbors import KNeighborsClassifier\n    from sklearn.svm\
            \ import SVC\n    from sklearn.naive_bayes import GaussianNB\n    '''\n\
            \n    block2 = '''\n    gaussian = GaussianNB()\n    gaussian.fit(train_df,\
            \ train_labels)\n    acc_gaussian = round(gaussian.score(train_df, train_labels)\
            \ * 100, 2)\n    '''\n\n    data_saving_block = '''\n    # -----------------------DATA\
            \ SAVING START---------------------------------\n    from kale.marshal\
            \ import utils as _kale_marshal_utils\n    _kale_marshal_utils.set_kale_data_directory(\"\
            /marshal\")\n    _kale_marshal_utils.save(acc_gaussian, \"acc_gaussian\"\
            )\n    # -----------------------DATA SAVING END-----------------------------------\n\
            \    '''\n\n    # run the code blocks inside a jupyter kernel\n    from\
            \ kale.utils.jupyter_utils import run_code as _kale_run_code\n    from\
            \ kale.utils.kfp_utils import \\\n        update_uimetadata as _kale_update_uimetadata\n\
            \    blocks = (data_loading_block,\n              block1,\n          \
            \    block2,\n              data_saving_block)\n    html_artifact = _kale_run_code(blocks)\n\
            \    with open(\"/naivebayes.html\", \"w\") as f:\n        f.write(html_artifact)\n\
            \    _kale_update_uimetadata('naivebayes')\n\n    _kale_mlmd_utils.call(\"\
            mark_execution_complete\")\n\nimport argparse\n_parser = argparse.ArgumentParser(prog='Naivebayes',\
            \ description='')\n_parsed_args = vars(_parser.parse_args())\n\n_outputs\
            \ = naivebayes(**_parsed_args)\n"
          image: python:3.7
          name: main
          securityContext:
            runAsUser: 0
          volumeMounts:
          - mountPath: /marshal
            name: kale-marshal-volume
          workingDir: /Users/animeshsingh/go/src/github.com/kubeflow/kale/examples/titanic-ml-dataset
        - env:
          - name: PIPELINERUN
            valueFrom:
              fieldRef:
                fieldPath: metadata.labels['tekton.dev/pipelineRun']
          - name: PIPELINETASK
            valueFrom:
              fieldRef:
                fieldPath: metadata.labels['tekton.dev/pipelineTask']
          - name: ARTIFACT_ENDPOINT_SCHEME
            valueFrom:
              fieldRef:
                fieldPath: metadata.annotations['tekton.dev/artifact_endpoint_scheme']
          - name: ARTIFACT_ENDPOINT
            valueFrom:
              fieldRef:
                fieldPath: metadata.annotations['tekton.dev/artifact_endpoint']
          - name: ARTIFACT_BUCKET
            valueFrom:
              fieldRef:
                fieldPath: metadata.annotations['tekton.dev/artifact_bucket']
          - name: AWS_ACCESS_KEY_ID
            valueFrom:
              secretKeyRef:
                key: accesskey
                name: mlpipeline-minio-artifact
          - name: AWS_SECRET_ACCESS_KEY
            valueFrom:
              secretKeyRef:
                key: secretkey
                name: mlpipeline-minio-artifact
          image: minio/mc
          name: copy-artifacts
          script: '#!/usr/bin/env sh

            mc config host add storage ${ARTIFACT_ENDPOINT_SCHEME}${ARTIFACT_ENDPOINT}
            $AWS_ACCESS_KEY_ID $AWS_SECRET_ACCESS_KEY

            tar -cvzf mlpipeline-ui-metadata.tgz /mlpipeline-ui-metadata.json

            mc cp mlpipeline-ui-metadata.tgz storage/$ARTIFACT_BUCKET/artifacts/$PIPELINERUN/$PIPELINETASK/mlpipeline-ui-metadata.tgz

            tar -cvzf naivebayes.tgz /naivebayes.html

            mc cp naivebayes.tgz storage/$ARTIFACT_BUCKET/artifacts/$PIPELINERUN/$PIPELINETASK/naivebayes.tgz

            '
        volumes:
        - name: kale-marshal-volume
          persistentVolumeClaim:
            claimName: $(inputs.params.kale-marshal-volume-name)
    - name: logisticregression
      params:
      - name: kale-marshal-volume-name
        value: $(tasks.kale-marshal-volume.results.name)
      runAfter:
      - featureengineering
      taskSpec:
        params:
        - name: kale-marshal-volume-name
        stepTemplate:
          volumeMounts:
          - mountPath: ''
            name: mlpipeline-ui-metadata
        steps:
        - command:
          - python3
          - -u
          - -c
          - "def logisticregression():\n    from kale.utils import mlmd_utils as _kale_mlmd_utils\n\
            \    _kale_mlmd_utils.init_metadata()\n\n    data_loading_block = '''\n\
            \    # -----------------------DATA LOADING START--------------------------------\n\
            \    from kale.marshal import utils as _kale_marshal_utils\n    _kale_marshal_utils.set_kale_data_directory(\"\
            /marshal\")\n    _kale_marshal_utils.set_kale_directory_file_names()\n\
            \    train_df = _kale_marshal_utils.load(\"train_df\")\n    train_labels\
            \ = _kale_marshal_utils.load(\"train_labels\")\n    # -----------------------DATA\
            \ LOADING END----------------------------------\n    '''\n\n    block1\
            \ = '''\n    import numpy as np\n    import pandas as pd\n    import seaborn\
            \ as sns\n    from matplotlib import pyplot as plt\n    from matplotlib\
            \ import style\n\n    from sklearn import linear_model\n    from sklearn.linear_model\
            \ import LogisticRegression\n    from sklearn.ensemble import RandomForestClassifier\n\
            \    from sklearn.linear_model import Perceptron\n    from sklearn.linear_model\
            \ import SGDClassifier\n    from sklearn.tree import DecisionTreeClassifier\n\
            \    from sklearn.neighbors import KNeighborsClassifier\n    from sklearn.svm\
            \ import SVC\n    from sklearn.naive_bayes import GaussianNB\n    '''\n\
            \n    block2 = '''\n    logreg = LogisticRegression(solver='lbfgs', max_iter=110)\n\
            \    logreg.fit(train_df, train_labels)\n    acc_log = round(logreg.score(train_df,\
            \ train_labels) * 100, 2)\n    '''\n\n    data_saving_block = '''\n  \
            \  # -----------------------DATA SAVING START---------------------------------\n\
            \    from kale.marshal import utils as _kale_marshal_utils\n    _kale_marshal_utils.set_kale_data_directory(\"\
            /marshal\")\n    _kale_marshal_utils.save(acc_log, \"acc_log\")\n    #\
            \ -----------------------DATA SAVING END-----------------------------------\n\
            \    '''\n\n    # run the code blocks inside a jupyter kernel\n    from\
            \ kale.utils.jupyter_utils import run_code as _kale_run_code\n    from\
            \ kale.utils.kfp_utils import \\\n        update_uimetadata as _kale_update_uimetadata\n\
            \    blocks = (data_loading_block,\n              block1,\n          \
            \    block2,\n              data_saving_block)\n    html_artifact = _kale_run_code(blocks)\n\
            \    with open(\"/logisticregression.html\", \"w\") as f:\n        f.write(html_artifact)\n\
            \    _kale_update_uimetadata('logisticregression')\n\n    _kale_mlmd_utils.call(\"\
            mark_execution_complete\")\n\nimport argparse\n_parser = argparse.ArgumentParser(prog='Logisticregression',\
            \ description='')\n_parsed_args = vars(_parser.parse_args())\n\n_outputs\
            \ = logisticregression(**_parsed_args)\n"
          image: python:3.7
          name: main
          securityContext:
            runAsUser: 0
          volumeMounts:
          - mountPath: /marshal
            name: kale-marshal-volume
          workingDir: /Users/animeshsingh/go/src/github.com/kubeflow/kale/examples/titanic-ml-dataset
        - env:
          - name: PIPELINERUN
            valueFrom:
              fieldRef:
                fieldPath: metadata.labels['tekton.dev/pipelineRun']
          - name: PIPELINETASK
            valueFrom:
              fieldRef:
                fieldPath: metadata.labels['tekton.dev/pipelineTask']
          - name: ARTIFACT_ENDPOINT_SCHEME
            valueFrom:
              fieldRef:
                fieldPath: metadata.annotations['tekton.dev/artifact_endpoint_scheme']
          - name: ARTIFACT_ENDPOINT
            valueFrom:
              fieldRef:
                fieldPath: metadata.annotations['tekton.dev/artifact_endpoint']
          - name: ARTIFACT_BUCKET
            valueFrom:
              fieldRef:
                fieldPath: metadata.annotations['tekton.dev/artifact_bucket']
          - name: AWS_ACCESS_KEY_ID
            valueFrom:
              secretKeyRef:
                key: accesskey
                name: mlpipeline-minio-artifact
          - name: AWS_SECRET_ACCESS_KEY
            valueFrom:
              secretKeyRef:
                key: secretkey
                name: mlpipeline-minio-artifact
          image: minio/mc
          name: copy-artifacts
          script: '#!/usr/bin/env sh

            mc config host add storage ${ARTIFACT_ENDPOINT_SCHEME}${ARTIFACT_ENDPOINT}
            $AWS_ACCESS_KEY_ID $AWS_SECRET_ACCESS_KEY

            tar -cvzf mlpipeline-ui-metadata.tgz /mlpipeline-ui-metadata.json

            mc cp mlpipeline-ui-metadata.tgz storage/$ARTIFACT_BUCKET/artifacts/$PIPELINERUN/$PIPELINETASK/mlpipeline-ui-metadata.tgz

            tar -cvzf logisticregression.tgz /logisticregression.html

            mc cp logisticregression.tgz storage/$ARTIFACT_BUCKET/artifacts/$PIPELINERUN/$PIPELINETASK/logisticregression.tgz

            '
        volumes:
        - name: kale-marshal-volume
          persistentVolumeClaim:
            claimName: $(inputs.params.kale-marshal-volume-name)
    - name: randomforest
      params:
      - name: kale-marshal-volume-name
        value: $(tasks.kale-marshal-volume.results.name)
      runAfter:
      - featureengineering
      taskSpec:
        params:
        - name: kale-marshal-volume-name
        stepTemplate:
          volumeMounts:
          - mountPath: ''
            name: mlpipeline-ui-metadata
        steps:
        - command:
          - python3
          - -u
          - -c
          - "def randomforest():\n    from kale.utils import mlmd_utils as _kale_mlmd_utils\n\
            \    _kale_mlmd_utils.init_metadata()\n\n    data_loading_block = '''\n\
            \    # -----------------------DATA LOADING START--------------------------------\n\
            \    from kale.marshal import utils as _kale_marshal_utils\n    _kale_marshal_utils.set_kale_data_directory(\"\
            /marshal\")\n    _kale_marshal_utils.set_kale_directory_file_names()\n\
            \    train_df = _kale_marshal_utils.load(\"train_df\")\n    train_labels\
            \ = _kale_marshal_utils.load(\"train_labels\")\n    # -----------------------DATA\
            \ LOADING END----------------------------------\n    '''\n\n    block1\
            \ = '''\n    import numpy as np\n    import pandas as pd\n    import seaborn\
            \ as sns\n    from matplotlib import pyplot as plt\n    from matplotlib\
            \ import style\n\n    from sklearn import linear_model\n    from sklearn.linear_model\
            \ import LogisticRegression\n    from sklearn.ensemble import RandomForestClassifier\n\
            \    from sklearn.linear_model import Perceptron\n    from sklearn.linear_model\
            \ import SGDClassifier\n    from sklearn.tree import DecisionTreeClassifier\n\
            \    from sklearn.neighbors import KNeighborsClassifier\n    from sklearn.svm\
            \ import SVC\n    from sklearn.naive_bayes import GaussianNB\n    '''\n\
            \n    block2 = '''\n    random_forest = RandomForestClassifier(n_estimators=100)\n\
            \    random_forest.fit(train_df, train_labels)\n    acc_random_forest\
            \ = round(random_forest.score(train_df, train_labels) * 100, 2)\n    '''\n\
            \n    data_saving_block = '''\n    # -----------------------DATA SAVING\
            \ START---------------------------------\n    from kale.marshal import\
            \ utils as _kale_marshal_utils\n    _kale_marshal_utils.set_kale_data_directory(\"\
            /marshal\")\n    _kale_marshal_utils.save(acc_random_forest, \"acc_random_forest\"\
            )\n    # -----------------------DATA SAVING END-----------------------------------\n\
            \    '''\n\n    # run the code blocks inside a jupyter kernel\n    from\
            \ kale.utils.jupyter_utils import run_code as _kale_run_code\n    from\
            \ kale.utils.kfp_utils import \\\n        update_uimetadata as _kale_update_uimetadata\n\
            \    blocks = (data_loading_block,\n              block1,\n          \
            \    block2,\n              data_saving_block)\n    html_artifact = _kale_run_code(blocks)\n\
            \    with open(\"/randomforest.html\", \"w\") as f:\n        f.write(html_artifact)\n\
            \    _kale_update_uimetadata('randomforest')\n\n    _kale_mlmd_utils.call(\"\
            mark_execution_complete\")\n\nimport argparse\n_parser = argparse.ArgumentParser(prog='Randomforest',\
            \ description='')\n_parsed_args = vars(_parser.parse_args())\n\n_outputs\
            \ = randomforest(**_parsed_args)\n"
          image: python:3.7
          name: main
          securityContext:
            runAsUser: 0
          volumeMounts:
          - mountPath: /marshal
            name: kale-marshal-volume
          workingDir: /Users/animeshsingh/go/src/github.com/kubeflow/kale/examples/titanic-ml-dataset
        - env:
          - name: PIPELINERUN
            valueFrom:
              fieldRef:
                fieldPath: metadata.labels['tekton.dev/pipelineRun']
          - name: PIPELINETASK
            valueFrom:
              fieldRef:
                fieldPath: metadata.labels['tekton.dev/pipelineTask']
          - name: ARTIFACT_ENDPOINT_SCHEME
            valueFrom:
              fieldRef:
                fieldPath: metadata.annotations['tekton.dev/artifact_endpoint_scheme']
          - name: ARTIFACT_ENDPOINT
            valueFrom:
              fieldRef:
                fieldPath: metadata.annotations['tekton.dev/artifact_endpoint']
          - name: ARTIFACT_BUCKET
            valueFrom:
              fieldRef:
                fieldPath: metadata.annotations['tekton.dev/artifact_bucket']
          - name: AWS_ACCESS_KEY_ID
            valueFrom:
              secretKeyRef:
                key: accesskey
                name: mlpipeline-minio-artifact
          - name: AWS_SECRET_ACCESS_KEY
            valueFrom:
              secretKeyRef:
                key: secretkey
                name: mlpipeline-minio-artifact
          image: minio/mc
          name: copy-artifacts
          script: '#!/usr/bin/env sh

            mc config host add storage ${ARTIFACT_ENDPOINT_SCHEME}${ARTIFACT_ENDPOINT}
            $AWS_ACCESS_KEY_ID $AWS_SECRET_ACCESS_KEY

            tar -cvzf mlpipeline-ui-metadata.tgz /mlpipeline-ui-metadata.json

            mc cp mlpipeline-ui-metadata.tgz storage/$ARTIFACT_BUCKET/artifacts/$PIPELINERUN/$PIPELINETASK/mlpipeline-ui-metadata.tgz

            tar -cvzf randomforest.tgz /randomforest.html

            mc cp randomforest.tgz storage/$ARTIFACT_BUCKET/artifacts/$PIPELINERUN/$PIPELINETASK/randomforest.tgz

            '
        volumes:
        - name: kale-marshal-volume
          persistentVolumeClaim:
            claimName: $(inputs.params.kale-marshal-volume-name)
    - name: results
      params:
      - name: kale-marshal-volume-name
        value: $(tasks.kale-marshal-volume.results.name)
      runAfter:
      - randomforest
      - logisticregression
      - naivebayes
      - svm
      - decisiontree
      taskSpec:
        params:
        - name: kale-marshal-volume-name
        stepTemplate:
          volumeMounts:
          - mountPath: ''
            name: mlpipeline-ui-metadata
        steps:
        - command:
          - python3
          - -u
          - -c
          - "def results():\n    from kale.utils import mlmd_utils as _kale_mlmd_utils\n\
            \    _kale_mlmd_utils.init_metadata()\n\n    data_loading_block = '''\n\
            \    # -----------------------DATA LOADING START--------------------------------\n\
            \    from kale.marshal import utils as _kale_marshal_utils\n    _kale_marshal_utils.set_kale_data_directory(\"\
            /marshal\")\n    _kale_marshal_utils.set_kale_directory_file_names()\n\
            \    acc_decision_tree = _kale_marshal_utils.load(\"acc_decision_tree\"\
            )\n    acc_gaussian = _kale_marshal_utils.load(\"acc_gaussian\")\n   \
            \ acc_linear_svc = _kale_marshal_utils.load(\"acc_linear_svc\")\n    acc_log\
            \ = _kale_marshal_utils.load(\"acc_log\")\n    acc_random_forest = _kale_marshal_utils.load(\"\
            acc_random_forest\")\n    # -----------------------DATA LOADING END----------------------------------\n\
            \    '''\n\n    block1 = '''\n    import numpy as np\n    import pandas\
            \ as pd\n    import seaborn as sns\n    from matplotlib import pyplot\
            \ as plt\n    from matplotlib import style\n\n    from sklearn import\
            \ linear_model\n    from sklearn.linear_model import LogisticRegression\n\
            \    from sklearn.ensemble import RandomForestClassifier\n    from sklearn.linear_model\
            \ import Perceptron\n    from sklearn.linear_model import SGDClassifier\n\
            \    from sklearn.tree import DecisionTreeClassifier\n    from sklearn.neighbors\
            \ import KNeighborsClassifier\n    from sklearn.svm import SVC\n    from\
            \ sklearn.naive_bayes import GaussianNB\n    '''\n\n    block2 = '''\n\
            \    results = pd.DataFrame({\n        'Model': ['Support Vector Machines',\
            \ 'logistic Regression',\n                  'Random Forest', 'Naive Bayes',\
            \ 'Decision Tree'],\n        'Score': [acc_linear_svc, acc_log,\n    \
            \              acc_random_forest, acc_gaussian, acc_decision_tree]})\n\
            \    result_df = results.sort_values(by='Score', ascending=False)\n  \
            \  result_df = result_df.set_index('Score')\n    print(result_df)\n  \
            \  '''\n\n    # run the code blocks inside a jupyter kernel\n    from\
            \ kale.utils.jupyter_utils import run_code as _kale_run_code\n    from\
            \ kale.utils.kfp_utils import \\\n        update_uimetadata as _kale_update_uimetadata\n\
            \    blocks = (data_loading_block,\n              block1,\n          \
            \    block2,\n              )\n    html_artifact = _kale_run_code(blocks)\n\
            \    with open(\"/results.html\", \"w\") as f:\n        f.write(html_artifact)\n\
            \    _kale_update_uimetadata('results')\n\n    _kale_mlmd_utils.call(\"\
            mark_execution_complete\")\n\nimport argparse\n_parser = argparse.ArgumentParser(prog='Results',\
            \ description='')\n_parsed_args = vars(_parser.parse_args())\n\n_outputs\
            \ = results(**_parsed_args)\n"
          image: python:3.7
          name: main
          securityContext:
            runAsUser: 0
          volumeMounts:
          - mountPath: /marshal
            name: kale-marshal-volume
          workingDir: /Users/animeshsingh/go/src/github.com/kubeflow/kale/examples/titanic-ml-dataset
        - env:
          - name: PIPELINERUN
            valueFrom:
              fieldRef:
                fieldPath: metadata.labels['tekton.dev/pipelineRun']
          - name: PIPELINETASK
            valueFrom:
              fieldRef:
                fieldPath: metadata.labels['tekton.dev/pipelineTask']
          - name: ARTIFACT_ENDPOINT_SCHEME
            valueFrom:
              fieldRef:
                fieldPath: metadata.annotations['tekton.dev/artifact_endpoint_scheme']
          - name: ARTIFACT_ENDPOINT
            valueFrom:
              fieldRef:
                fieldPath: metadata.annotations['tekton.dev/artifact_endpoint']
          - name: ARTIFACT_BUCKET
            valueFrom:
              fieldRef:
                fieldPath: metadata.annotations['tekton.dev/artifact_bucket']
          - name: AWS_ACCESS_KEY_ID
            valueFrom:
              secretKeyRef:
                key: accesskey
                name: mlpipeline-minio-artifact
          - name: AWS_SECRET_ACCESS_KEY
            valueFrom:
              secretKeyRef:
                key: secretkey
                name: mlpipeline-minio-artifact
          image: minio/mc
          name: copy-artifacts
          script: '#!/usr/bin/env sh

            mc config host add storage ${ARTIFACT_ENDPOINT_SCHEME}${ARTIFACT_ENDPOINT}
            $AWS_ACCESS_KEY_ID $AWS_SECRET_ACCESS_KEY

            tar -cvzf mlpipeline-ui-metadata.tgz /mlpipeline-ui-metadata.json

            mc cp mlpipeline-ui-metadata.tgz storage/$ARTIFACT_BUCKET/artifacts/$PIPELINERUN/$PIPELINETASK/mlpipeline-ui-metadata.tgz

            tar -cvzf results.tgz /results.html

            mc cp results.tgz storage/$ARTIFACT_BUCKET/artifacts/$PIPELINERUN/$PIPELINETASK/results.tgz

            '
        volumes:
        - name: kale-marshal-volume
          persistentVolumeClaim:
            claimName: $(inputs.params.kale-marshal-volume-name)
